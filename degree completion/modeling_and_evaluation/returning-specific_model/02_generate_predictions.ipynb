{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats.mstats import gmean\n",
    "import math\n",
    "import datetime as dt\n",
    "import matplotlib\n",
    "font = {'size': 24}\n",
    "matplotlib.rc('font', **font)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-first-term-specific models:\n",
      "294\n"
     ]
    }
   ],
   "source": [
    "print(\"Non-first-term-specific models:\")\n",
    "df_new = pd.read_stata(\"../degree_completion_1/full_data_truncated_enlarged_new.dta\")\n",
    "df_new.loc[:,'available_sum'] = 0\n",
    "for p in [p for p in list(df_new.columns)[10:] if p.startswith(\"available\") and p != \"available_sum\"]:\n",
    "    df_new.loc[:,'available_sum'] += df_new[p]\n",
    "df_new = df_new[df_new.available_sum > 1]\n",
    "df_new = df_new.drop(['available_sum'], axis=1)\n",
    "\n",
    "predictors = pickle.load(open(\"../degree_completion_1/predictors_rf2.p\", \"rb\"))\n",
    "\n",
    "\n",
    "l1 = ['coll_lvl_cred_earn', 'prop_comp_pre', 'cum_gpa_pre']\n",
    "for p in l1:\n",
    "    v = np.min(df_new[p]) - 1\n",
    "    df_new.loc[:, p] = df_new.apply(lambda x: v if x['enrolled_pre'] != 1 else x[p], axis=1)\n",
    "l2 = ['admrate', 'gradrate', 'satvr25', 'satvr75', 'satmt25', 'satmt75', 'satwr25', 'satwr75', 'nsc_coll_type_1', 'nsc_coll_type_2', 'nsc_coll_type_3', 'nsc_coll_type_4', 'nsc_coll_type_5', 'nsc_coll_type_6', 'nsc_coll_type_7', 'nsc_coll_type_8']\n",
    "for p in l2:\n",
    "    v = np.min(df_new[p]) - 1\n",
    "    df_new.loc[:, p] = df_new.apply(lambda x: v if x['enrolled_nsc'] != 1 else x[p], axis=1)\n",
    "l3 = ['degree_seeking', 'term_cred_att', 'term_gpa', 'prop_comp', 'withdrawn_prop_comp', 'lvl2_prop_comp', 'dev_prop_comp', 'repeat']\n",
    "for t in ['su', 'fa', 'sp']:\n",
    "    for i in range(1,7):\n",
    "        for pp in l3:\n",
    "            p = pp + \"_\" + t + str(i)\n",
    "            v = np.min(df_new[p]) - 1\n",
    "            df_new.loc[:, p] = df_new.apply(lambda x: v if x['enrolled_' + t + str(i)] != 1 else x[p], axis=1)\n",
    "l4 = ['enrl_intensity_nsc']\n",
    "for t in ['su', 'fa', 'sp']:\n",
    "    for i in range(1,7):\n",
    "        for pp in l4:\n",
    "            p = pp + \"_\" + t + str(i)\n",
    "            v = np.min(df_new[p]) - 1\n",
    "            df_new.loc[:, p] = df_new.apply(lambda x: v if x['enrolled_nsc_' + t + str(i)] != 1 else x[p], axis=1)\n",
    "l5 = ['grants', 'sub_loans', 'unsub_loans', 'others']\n",
    "for i in range(1,7):\n",
    "    for pp in l5:\n",
    "        p = pp + \"_yr\" + str(i)\n",
    "        v = np.min(df_new[p]) - 1\n",
    "        df_new.loc[:, p] = df_new.apply(lambda x: v if x['enrolled_yr' + str(i)] != 1 else x[p], axis=1)\n",
    "to_drop = ['enrolled_pre', 'enrolled_nsc'] + ['enrolled_' + t + str(i) for t in ['su', 'fa', 'sp'] for i in range(1,7)] + ['enrolled_nsc_' + t + str(i) for t in ['su', 'fa', 'sp'] for i in range(1,7)]\n",
    "predictors = [p for p in predictors if p not in set(to_drop)]\n",
    "print(len(predictors))\n",
    "\n",
    "\n",
    "train_df = df_new[df_new.valid == 0]\n",
    "test_df = df_new[df_new.valid == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:,['vccsid']].to_stata(\"train_id_return.dta\", write_index=False)\n",
    "test_df.loc[:,['vccsid']].to_stata(\"test_id_return.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cw(y):\n",
    "    # Calculate the weight of each letter grade to be used in the modeling fitting procedure: the weight is inversely proportional to the square root of the frequency of the letter grade in the training sample\n",
    "    cw = Counter(y)\n",
    "    class_weight = {k:np.sqrt(cw.most_common()[0][-1]/v, dtype=np.float32) for k,v in cw.items()}\n",
    "    return class_weight # The output is a dictionary mapping letter grade to the corresponding weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True,\n",
       "            class_weight={0.0: 1.0, 1.0: 1.2543377}, criterion='entropy',\n",
       "            max_depth=15, max_features=12, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=140, n_jobs=-1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_d = 15\n",
    "optimal_n = 140\n",
    "optimal_nf = 12\n",
    "rf = RandomForestClassifier(n_estimators=optimal_n, criterion=\"entropy\",\n",
    "                            max_depth=optimal_d,\n",
    "                            random_state=0, n_jobs=-1, max_features=optimal_nf,\n",
    "                            class_weight = calc_cw(train_df.grad_6years))\n",
    "rf.fit(train_df.loc[:,predictors], train_df.grad_6years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "Validation AUC = 0.8932\n",
      "Training AUC = 0.9248\n"
     ]
    }
   ],
   "source": [
    "# Coefficients and predicted scores\n",
    "y_test_pred_rf = rf.predict_proba(test_df.loc[:,predictors])[:,1]\n",
    "y_train_pred_rf = rf.predict_proba(train_df.loc[:,predictors])[:,1]\n",
    "pickle.dump(y_test_pred_rf, open(\"y_test_pred_rf_return.p\", \"wb\"))\n",
    "pickle.dump(list(test_df.grad_6years), open( \"y_test_return.p\", \"wb\"))\n",
    "pickle.dump(y_train_pred_rf, open(\"y_train_pred_rf_return.p\", \"wb\"))\n",
    "pickle.dump(list(train_df.grad_6years), open(\"y_train_return.p\", \"wb\"))\n",
    "print(\"Random Forest:\")\n",
    "print(\"Validation AUC = {}\".format(round(roc_auc_score(test_df.grad_6years, y_test_pred_rf),4)))\n",
    "print(\"Training AUC = {}\".format(round(roc_auc_score(train_df.grad_6years, y_train_pred_rf),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47371 47371 47371\n"
     ]
    }
   ],
   "source": [
    "race_column = []\n",
    "for i in range(test_df.shape[0]):\n",
    "    if test_df.white.iloc[i] == 1:\n",
    "        race_column.append(\"white\")\n",
    "    elif test_df.afam.iloc[i] == 1:\n",
    "        race_column.append(\"afam\")\n",
    "    elif test_df.hisp.iloc[i] == 1:\n",
    "        race_column.append(\"hisp\")\n",
    "    elif test_df.asian.iloc[i] == 1:\n",
    "        race_column.append(\"asian\")\n",
    "    elif test_df.other.iloc[i] == 1:\n",
    "        race_column.append(\"other\")\n",
    "    else:\n",
    "        race_column.append(\"mi\")\n",
    "race_column = np.array(race_column)\n",
    "pred_y = np.array(y_test_pred_rf)\n",
    "test_y = np.array(test_df.grad_6years)\n",
    "pred_y = pred_y[race_column != \"mi\"]\n",
    "test_y = test_y[race_column != \"mi\"]\n",
    "race_column = race_column[race_column != \"mi\"]\n",
    "print(len(race_column), len(pred_y), len(test_y))\n",
    "\n",
    "pred_score_by_race = pd.DataFrame({'race_column': race_column, 'pred_y': pred_y, 'test_y': test_y})\n",
    "pred_score_by_race.to_csv(\"pred_score_by_race_return.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
