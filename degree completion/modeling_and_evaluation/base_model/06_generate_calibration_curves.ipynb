{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script generates the calibration curve plots (Figure 2), based on the base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats.mstats import gmean\n",
    "import math\n",
    "import datetime as dt\n",
    "import matplotlib\n",
    "font = {'size': 24}\n",
    "matplotlib.rc('font', **font)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new = pd.read_stata(\"df_new.dta\")\n",
    "predictors = pickle.load(open(\"predictors.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cw(y):\n",
    "    # Calculate the weight of each letter grade to be used in the modeling fitting procedure: the weight is inversely proportional to the square root of the frequency of the letter grade in the training sample\n",
    "    cw = Counter(y)\n",
    "    class_weight = {k:np.sqrt(cw.most_common()[0][-1]/v, dtype=np.float32) for k,v in cw.items()}\n",
    "    return class_weight # The output is a dictionary mapping letter grade to the corresponding weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323182 62618\n"
     ]
    }
   ],
   "source": [
    "train_df = df_new[df_new.valid == 0]\n",
    "test_df = df_new[df_new.valid == 1]\n",
    "print(train_df.shape[0], test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True,\n",
       "            class_weight={0.0: 1.0, 1.0: 1.3931639}, criterion='entropy',\n",
       "            max_depth=16, max_features=12, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=120, n_jobs=-1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_d = 16\n",
    "optimal_n = 120\n",
    "optimal_nf = 12\n",
    "rf = RandomForestClassifier(n_estimators=optimal_n, criterion=\"entropy\",\n",
    "                            max_depth=optimal_d,\n",
    "                            random_state=0, n_jobs=-1, max_features=optimal_nf,\n",
    "                            class_weight = calc_cw(train_df.grad_6years))\n",
    "rf.fit(train_df.loc[:,predictors], train_df.grad_6years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred_rf = rf.predict_proba(test_df.loc[:,predictors])[:,1]\n",
    "y_train_pred_rf = rf.predict_proba(train_df.loc[:,predictors])[:,1]\n",
    "pickle.dump(y_test_pred_rf, open(\"y_test_pred_rf.p\", \"wb\"))\n",
    "pickle.dump(list(test_df.grad_6years), open(\"y_test.p\", \"wb\"))\n",
    "pickle.dump(y_train_pred_rf, open(\"y_train_pred_rf.p\", \"wb\"))\n",
    "pickle.dump(list(train_df.grad_6years), open(\"y_train.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold = 0.5091275515621155\n"
     ]
    }
   ],
   "source": [
    "sr = sum(train_df.grad_6years)/train_df.shape[0]\n",
    "n = int(train_df.shape[0] - train_df.shape[0] * sr)\n",
    "best_threshold = sorted(y_train_pred_rf)[n-1]\n",
    "print(\"Best threshold = {}\".format(best_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "success_rate_1 = []\n",
    "for r in ['overall', 'white', 'afam', 'hisp', 'asian', 'other']:\n",
    "    if r == 'overall':\n",
    "        y_arr = np.array(y_test_pred_rf)\n",
    "        y_actual = np.array(test_df.grad_6years)\n",
    "    else:\n",
    "        y_arr = np.array(y_test_pred_rf)[np.where(np.array(test_df[r]) == 1)[0]]\n",
    "        y_actual = np.array(test_df.grad_6years)[np.where(np.array(test_df[r]) == 1)[0]]\n",
    "    success_rate_1.append((r, len(y_arr), np.mean(y_actual), np.mean(y_arr > best_threshold)))\n",
    "    fig = plt.figure(figsize=(16,11)) \n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    pd.DataFrame({r:y_arr}).hist(r, bins = np.linspace(0,1,26), density=True, color='orange', figsize=(16,11), ax=ax)\n",
    "    for p,q in zip(np.percentile(y_arr, q = [10,25,50,75,90]), [10,25,50,75,90]):\n",
    "        ax.axvline(x=p, color='g', linestyle='dashed', linewidth=2)\n",
    "        if r == \"afam\" and q == 10:\n",
    "            ax.text(p-0.045,ax.get_ylim()[1]*1.005,\"{}%\".format(q),fontsize=20)\n",
    "        elif r == \"afam\" and q == 25:\n",
    "            ax.text(p-0.008,ax.get_ylim()[1]*1.005,\"{}%\".format(q),fontsize=20)\n",
    "        else:\n",
    "            ax.text(p-0.02,ax.get_ylim()[1]*1.005,\"{}%\".format(q),fontsize=20)\n",
    "    ax.set_xlabel(\"Predicted Score\", fontsize=32)\n",
    "    ax.set_ylabel(\"Density\", fontsize=32)\n",
    "    if r != \"afam\":\n",
    "        ax.set_title(r.capitalize(), fontsize=40, pad=40)\n",
    "    else:\n",
    "        ax.set_title(\"Black\", fontsize=40, pad=40)\n",
    "    # plt.show()\n",
    "    plt.savefig(r +\"_1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62049 62049 62049\n"
     ]
    }
   ],
   "source": [
    "race_column = []\n",
    "for i in range(test_df.shape[0]):\n",
    "    if test_df.white.iloc[i] == 1:\n",
    "        race_column.append(\"white\")\n",
    "    elif test_df.afam.iloc[i] == 1:\n",
    "        race_column.append(\"afam\")\n",
    "    elif test_df.hisp.iloc[i] == 1:\n",
    "        race_column.append(\"hisp\")\n",
    "    elif test_df.asian.iloc[i] == 1:\n",
    "        race_column.append(\"asian\")\n",
    "    elif test_df.other.iloc[i] == 1:\n",
    "        race_column.append(\"other\")\n",
    "    else:\n",
    "        race_column.append(\"mi\")\n",
    "race_column = np.array(race_column)\n",
    "pred_y = np.array(y_test_pred_rf)\n",
    "test_y = np.array(test_df.grad_6years)\n",
    "pred_y = pred_y[race_column != \"mi\"]\n",
    "test_y = test_y[race_column != \"mi\"]\n",
    "race_column = race_column[race_column != \"mi\"]\n",
    "print(len(race_column), len(pred_y), len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_pred_real = pd.DataFrame({'pred_y': pred_y, 'real_y': test_y, 'race': race_column})\n",
    "try:\n",
    "    new_pred_real.loc[:,'pred_y_binned'] = pd.cut(new_pred_real.pred_y, bins=[0] + list(np.percentile(new_pred_real.pred_y, np.arange(2,100,2))) + [1])\n",
    "except ValueError:\n",
    "    new_pred_real.loc[:,'pred_y_binned'] = pd.cut(new_pred_real.pred_y.rank(method='first'), bins=[0] + list(np.percentile(new_pred_real.pred_y.rank(method='first'), np.arange(2,100,2))) + [new_pred_real.shape[0]+1])\n",
    "try:\n",
    "    new_pred_real.loc[:,'pred_y_binned_2'] = pd.cut(new_pred_real.pred_y, bins=[min(new_pred_real.pred_y) - 1e-3] + list(np.percentile(new_pred_real.pred_y, np.arange(10,100,10))) + [max(new_pred_real.pred_y) + 1e-3])\n",
    "except ValueError:\n",
    "    new_pred_real.loc[:,'pred_y_binned_2'] = pd.cut(new_pred_real.pred_y.rank(method='first'), bins=[0] + list(np.percentile(new_pred_real.pred_y.rank(method='first'), np.arange(10,100,10))) + [new_pred_real.shape[0]+1])\n",
    "pct_dict = {e:(10*indx+5) for indx, e in enumerate(sorted(list(Counter(new_pred_real.pred_y_binned_2).keys())))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afam\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "new_pred_real.loc[:,'real_y'] = new_pred_real.real_y * 100\n",
    "for r in ['afam']:\n",
    "    print(r)\n",
    "    new_sub = new_pred_real.copy()[new_pred_real.race.apply(lambda x: x in ['white', r])]\n",
    "    new_sub = new_sub.groupby(['pred_y_binned', 'race']).agg({'real_y':'mean'}).reset_index()\n",
    "    new_sub.loc[:,r] = new_sub.race.apply(lambda x: 1 if x == r else 0)\n",
    "    new_sub = new_sub.sort_values([r, 'pred_y_binned'])\n",
    "    print(new_sub.shape[0])\n",
    "    new_sub.loc[:,'pred_score_percentile'] = list(np.linspace(1,99,50))*2\n",
    "    new_sub = new_sub.rename(columns={'real_y':'share_of_actual_ABC'}).drop(['pred_y_binned'], axis=1)\n",
    "\n",
    "    sns.set_style(style = \"darkgrid\")\n",
    "    fig, ax = plt.subplots(1,1, figsize=(24,16.5))\n",
    "    sns.scatterplot(x=\"pred_score_percentile\", y=\"share_of_actual_ABC\", hue='race', hue_order = ['white', r],\n",
    "                    data=new_sub,\n",
    "                    palette = ['C0','C2'], marker=\"x\", ax=ax, s=150, alpha=0.7, linewidth = 3)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles=handles[1:], labels=['White', 'Black'], fontsize='40', markerscale=3)\n",
    "    plt.xticks(np.linspace(0,100,11),fontsize=36)\n",
    "    plt.yticks(np.linspace(0,100,11),fontsize=36)\n",
    "    \n",
    "    new_sub = new_pred_real.copy()[new_pred_real.race.apply(lambda x: x in ['white', r])]\n",
    "    new_sub.loc[:,'pred_score_percentile_new'] = new_sub.pred_y_binned_2.apply(lambda x: pct_dict[x])\n",
    "    np.random.seed(4321)\n",
    "    sns.lineplot(data=new_sub, x=\"pred_score_percentile_new\", y=\"real_y\", hue='race', hue_order = ['white', r],\n",
    "                 err_style=\"bars\", err_kws = {'capsize': 8, 'elinewidth':3, 'capthick':2}, \n",
    "                 ci=95, ax=ax, linewidth = 6,\n",
    "                 palette = ['C0','C2'], legend=False,\n",
    "                 marker=\".\", markersize=36)\n",
    "    plt.xlabel(\"Predicted Score Percentile\", fontsize=40, labelpad=16)\n",
    "    plt.ylabel(\"% of Students Who Earn Degree or Certificate in 6 Years\", fontsize=36, labelpad=16)\n",
    "    plt.savefig(r +\"_predictive_parity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
